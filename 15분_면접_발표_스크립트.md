# 100만 동시접속 실시간 시청자수 시스템 - 15분 면접 발표 스크립트


안녕하세요. 오늘은 100만 동시접속 실시간 시청자수 시스템 설계에 대해 발표하겠습니다.

## 1. 비즈니스 요구사항 분석과 TPS 산정 (2분)

설계를 시작하기 전에 가장 먼저 한 일은 정확한 TPS 산정이었습니다. 

라이브 스트리밍 플랫폼에서 평균 시청시간은 보통 15분 정도입니다. 이를 기반으로 계산해보면, 100만 동시접속자가 있을 때 시간당 회전율은 4회가 됩니다. 그러면 시간당 총 800만번의 입장과 퇴장이 발생하게 되죠.

이를 초당으로 환산하면 기본적으로 2,222건의 TPS가 나옵니다. 하지만 여기서 중요한 건 트래픽 집중 현상이었습니다. 

실제 라이브 스트리밍에서는 인기 방송에 트래픽이 몰리는 패턴이 있어서, 이를 고려해 3배 여유분을 둬서 피크타임 6,600 TPS로 설계했습니다.

그런데 여기서 더 큰 문제를 발견했습니다. 유명 스트리머가 갑자기 방송을 시작하면 5분 안에 10만명이 몰리는 상황이 발생합니다. 이때는 TPS가 25,000까지 급증하게 되는데, 


## 2. 시스템 아키텍처 소개 (2분)


먼저 전체 아키텍처를 보시면, 크게 3개 레이어로 구성되어 있습니다.

"100만 동시접속을 처리하기 위해 3층 구조로 설계했습니다"

첫 번째 층은 WebSocket 서버 풀입니다. 20대의 서버가 각각 5만 연결을 담당하여 총 100만 연결을 처리합니다.

1. 메모리 예산

웹소켓 하나가 커널 소켓, 버퍼, 세션 상태를 합쳐서 대략 10~15KB를 씁니다.

커널 소켓/버퍼: 운영체제가 연결당 기본으로 잡는 메모리 공간

세션 상태: 우리 서비스에서 유저 정보·메시지 큐 등을 저장하는 공간

우리는 TCP 버퍼 기본값(수백 KB)을 8~16KB로 낮췄습니다. (메시지 크기가 작아서 가능)
그럼에도 5만 × 12KB = 약 600MB는 연결에만 들어갑니다.

여기에 애플리케이션 힙(프로그램 실행 시 쓰는 메모리) 1.52GB,
OS/에이전트(리눅스·모니터링 에이전트 등) 11.5GB,
**파일 캐시와 스파이크 여유(예상치 못한 순간 부하)**까지 합치면,
8~16GB 서버에서 5만이 안정 상한입니다.

즉, 메모리를 60~70%만 쓰게 해서 **OOM(Out of Memory, 메모리 부족 에러)**이나
**GC 꼬리 지연(Garbage Collection이 오래 걸려 전체 서비스가 멈칫하는 현상)**을 피하는 겁니다.


------------------------------------------------------------------------------------------------------
추가 설명 
------------------------------------------------------------------------------------------------------

1. 메모리 예산 파트의 의미

핵심은 이거예요:
👉 서버 메모리 용량 안에서, 웹소켓 연결을 몇 개까지 안정적으로 유지할 수 있는지를 계산한 것

왜 필요하냐?

웹소켓 연결 하나가 단순히 “줄 하나”로 기록되는 게 아니고,
운영체제(OS)와 프로그램이 그 연결을 관리하기 위해 일정한 메모리를 계속 붙잡고 있어야 합니다.

즉, 사람 1명이 들어오면 자리(소켓 버퍼) + 짐칸(세션 데이터) 같은 게 필요하다는 거예요.
사람이 많아질수록 서버 메모리를 계속 갉아먹습니다.

숫자 계산 과정

연결 1개당 메모리 사용량: 10~15KB
(운영체제 TCP 버퍼 + 애플리케이션 세션 정보 합쳐서)

5만 개 연결 × 12KB = 약 600MB
👉 600MB는 연결 관리에만 계속 묶입니다.

그 외에 상시 필요한 메모리:

프로그램 자체 실행(애플리케이션 힙): 1.5~2GB

운영체제 + 모니터링 도구 등: 1~1.5GB

캐시와 여유 버퍼(스파이크 대비): 수 GB

즉, 8~16GB 서버라면 이미 절반 이상이 기본적으로 쓰이고,
남는 게 많지 않으니 5만 개 이상을 억지로 넣으면 불안정해진다는 겁니다.

“메모리 예산”이란?

서버 메모리를 예산 짜듯이 항목별로 나눠서 계산한 거예요.

연결 관리에 얼마

애플리케이션에 얼마

OS/도구에 얼마

예비 여유분 얼마

이렇게 계산해봤더니, 5만 개가 딱 안정적인 상한선이라는 결론이 나온 겁니다.







2. 이벤트 루프 / GC의 꼬리 지연 (p99)

이벤트 루프 / GC 꼬리 지연 (쉽게 설명)

핵심 메시지:
👉 평균은 괜찮아 보여도, **최악의 순간(꼬리 지연)**에 끊김이 생기기 때문에 5만으로 제한한 것.

비유로 먼저 설명

생각해보면, 도로가 평소엔 차가 잘 다니지만 퇴근 시간에 신호등·차량·사고가 한꺼번에 겹치면 정체가 심해지는 것과 같습니다.
웹소켓 서버도 비슷해서, 특정 순간에는 여러 이벤트가 동시에 몰리면서 지연이 확 튀어버려요.

용어 풀어서

이벤트 루프:
서버가 수만 개 연결을 동시에 관리할 때, 메시지·타이머 등을 줄 세워서 하나씩 빨리 처리하는 시스템.
(마치 “한 줄로 서세요”라고 안내하는 은행 창구 같은 느낌)

하트비트:
연결이 살아 있는지 주기적으로 확인하는 신호.
(환자가 살아있는지 확인하는 심장 박동 체크 같은 개념)

GC(Garbage Collection):
더 이상 안 쓰는 메모리를 자동으로 치우는 과정.
(집 청소 같은 개념, 평소엔 괜찮은데 대청소가 겹치면 서비스가 잠깐 멈칫)

p99 지연:
전체 요청 중 99%까지는 빠르게 처리되지만, 가장 느린 상위 1% 구간의 응답 시간.
(즉, 평균이 아니라 최악의 순간에 걸린 사용자 경험을 본 지표)

실제 의미

연결이 5만일 때는 이벤트 루프·하트비트·GC가 겹쳐도 지연이 일정하게 유지됩니다.

하지만 10만을 넘기면 평균은 버티지만, **p99.9에서 지연 스파이크(끊김)**가 반복됩니다.

우리는 **SLO(서비스 품질 목표)**를 “평균이 아니라 최악의 순간에도 끊기지 않게”로 잡았기 때문에, 5만으로 제한한 겁니다.

발표용 요약 대본 (30초)

“두 번째 이유는 지연 시간입니다.
평균만 보면 서버 한 대에 10만도 버틸 것 같지만, 실제 문제는 최악의 순간입니다.
이벤트 루프가 수만 개 메시지를 처리하고, 하트비트가 몰리고, 가비지 컬렉션이 동시에 일어나면 순간적으로 지연이 확 튀어버립니다.
5만 연결까지는 안정적이지만, 10만을 넘기면 p99 구간에서 끊김이 반복됩니다.
저희는 평균이 아니라 ‘최악의 순간에도 안정적일 것’을 목표로 해서 5만을 선택했습니다.”



꼬리질문 대비 카드

Q. 평균 처리량은 충분한데 왜 굳이 5만으로 제한했나요?
👉 평균은 버티지만, 이벤트 루프·하트비트·GC가 겹칠 때 p99 지연이 확 튑니다.
사용자는 평균이 아니라 최악의 순간 지연을 체감하기 때문에, 5만이 안정선입니다.

Q. GC 꼬리 지연이란 게 구체적으로 뭔가요?
👉 가비지 컬렉션은 안 쓰는 메모리를 치우는 작업인데, 그 순간 이벤트 처리가 잠시 멈칫합니다.
연결이 많을수록 그 영향이 커져서 p99 지연에 끊김이 생깁니다.

Q. 왜 p99 기준을 쓰나요? 평균이 더 중요하지 않나요?
👉 평균은 괜찮아도 상위 1% 구간에서 끊기면 실제 사용자 체감은 나빠집니다.
그래서 저희는 SLO를 평균이 아닌 p99 기준으로 잡았습니다.





3. 재접속 폭풍 모델 (쉽게 설명)


핵심 메시지

👉 모바일 환경에서는 수천 명이 동시에 연결이 끊겼다가 다시 몰려드는 순간이 생기는데, 서버당 연결을 5만 단위로 쪼개야 그 충격을 흡수할 수 있다.

상황 비유

지하철에 갑자기 수천 명이 한 번에 내리고 타려는 상황을 떠올려보세요.
출입문이 하나밖에 없다면 한 번에 밀려 들어와서 난리가 나지만,
출입문이 여러 개라면 사람들이 분산돼 훨씬 부드럽게 처리됩니다.

서버도 똑같습니다. 한 서버에 10만 명이 몰려 있으면, 그 순간 폭탄처럼 밀려 들어와 큐가 폭발합니다.
하지만 5만 × 20대로 나누면 그 폭탄이 20등분 돼서 매끄럽게 소화됩니다.

낯선 용어 풀이

accept 큐:
서버가 새 연결을 받을 때 잠시 줄 세워두는 대기열.
(놀이공원에서 입장 대기줄 같은 개념)

소프트IRQ(Soft Interrupt Request):
네트워크 패킷 같은 걸 OS가 빠르게 처리하는 경로.
(경찰관이 교통정리하는 것처럼 순간순간 일어나는 일을 빨리 처리해줌)

epoll 웨이크업:
리눅스에서 “소켓 이벤트가 생겼어! 처리해!” 하고 수만 개 연결을 깨우는 기능.
(강당에서 동시에 불 켜고 모두 깨우는 것과 비슷)

👉 이 세 가지가 동시에 몰리면 CPU와 큐가 한꺼번에 과부하 됩니다.

배포 시 드레인(Drain)

배포할 때는 서버를 교체해야 합니다.
이때 한 서버를 로드밸런서에서 빼면, 연결된 사용자들이 하나씩 빠져나가야 해요.

서버에 10만 명이 붙어 있다면 빠져나가는 데 시간이 오래 걸리고 불균형이 생깁니다.

서버에 5만 명 단위라면 훨씬 고르게 빠져나가서 배포 시 안정성도 좋아집니다.

발표용 요약 대본 (30초)

“세 번째 이유는 재접속 폭풍입니다.
모바일 환경에서는 와이파이가 끊겼다가 붙으면서 수천 명이 동시에 재접속하는 경우가 많습니다.
만약 서버에 10만 명이 붙어 있다면 이 재접속 폭풍이 한 대로 몰려 accept 큐, 네트워크 인터럽트, epoll 이벤트가 한 번에 터지면서 과부하가 납니다.
하지만 서버당 5만 명 단위로 20대를 나누면 같은 폭풍도 20등분되어 각 서버가 자연스럽게 흡수합니다.
배포할 때도 드레인이 훨씬 균일하고 안정적으로 진행됩니다.”


꼬리질문 대비 카드

Q. 재접속 폭풍이 왜 그렇게 큰 문제가 되나요?
👉 모바일 환경에서는 와이파이·5G 전환 같은 이유로 수천 명이 동시에 끊겼다가 다시 붙습니다.
이게 한 서버로 몰리면 큐와 네트워크 이벤트가 폭발하면서 순간적으로 과부하가 납니다.

Q. 왜 10만 연결보다 5만 연결이 안전한가요?
👉 10만이 한 서버에 몰려 있으면 폭탄처럼 한 번에 터지지만,
5만 × 20대로 쪼개면 같은 폭탄도 20등분되어 각 서버가 부드럽게 흡수합니다.

Q. 드레인(Drain)이 뭐고 왜 중요한가요?
👉 배포할 때 서버를 교체하려면 연결된 유저들을 빼내야 합니다.
서버당 10만 명이면 오래 걸리고 불균형이 생기지만,
5만 단위면 훨씬 빨리 균일하게 빠져 안정적인 배포가 가능합니다.






4. 장애 반경과 재수용 헤드룸 (쉽게 설명)
핵심 메시지

👉 서버가 죽더라도 영향을 최소화하려면 서버당 연결 수를 작게 쪼개는 게 안전하다.

비유

한 반에 학생이 100명 있는 반이 있고, 50명 있는 반이 있다고 해봅시다.
선생님이 갑자기 아프셔서 수업을 못 하게 되면,

100명 반은 갑자기 학생 100명이 전부 다른 반으로 흩어져야 해서 혼란이 큽니다.

50명 반은 절반만 옮기면 되니까 다른 반이 부담 없이 받아줄 수 있습니다.

서버도 똑같습니다.

용어 풀이

장애 반경 (Blast Radius)
→ 서버가 죽었을 때 영향을 받는 최대 인원.
(즉, “터졌을 때 얼마나 많은 유저가 휘말리냐”의 범위)

N-1, N-2
→ 서버 20대 중에서 한 대(N-1) 또는 두 대(N-2)가 죽은 상황.
(N은 전체 서버 개수를 뜻하는 기호입니다)

재수용 헤드룸 (Headroom)
→ 남아 있는 서버들이 추가로 받아줄 수 있는 여유 공간.
(예: 서버당 80%까지만 채워놓으면, 장애 시 20% 여유로 다른 서버 유저를 흡수할 수 있음)

실제 의미

서버당 5만으로 나누면 → 한 대가 죽어도 최대 5만 명만 재접속.

나머지 서버 19대가 각자 조금씩 여유 공간을 써서 금방 분산해 처리 가능.

반대로 서버당 10만이면 → 한 대만 죽어도 10만 명이 한 번에 몰려서 다른 서버들이 버거워짐.

발표용 요약 대본 (30초)

“마지막 이유는 장애 범위와 재수용 여유 공간입니다.
서버 한 대가 죽었을 때 그 서버 인원만 재연결하면 되는데, 5만 단위면 최대 영향 인원이 제한됩니다.
또 나머지 서버들이 미리 남겨둔 여유 공간을 활용해 쉽게 분산할 수 있습니다.
이걸 N-1, N-2 상황이라고 부르는데, 20대 중 한두 대가 빠져도 전체 서비스가 안정적으로 유지됩니다.
결국 서버를 작게 쪼갤수록 장애 영향은 줄고, 복구는 훨씬 빨라집니다.”



한 줄 결론

“5만/대는 성능을 낮춘 게 아니라, 최악의 순간에도 끊기지 않게 하기 위한 최적점입니다.
메모리, 지연 시간, 재접속 폭풍, 장애 반경을 모두 고려한 결과입니다.”


꼬리질문 대비 (쉽게)

Q. 더 큰 서버 쓰면 서버 수 줄일 수 있지 않나요?
→ 가능하지만, 한 대 죽었을 때 피해가 커집니다. 작은 서버 여러 대가 더 안정적입니다.

Q. 파일 디스크립터/커널 튜닝은 뭔가요?
→ **파일 디스크립터(FD)**는 연결 1개마다 필요한 번호표 같은 겁니다.
기본 한계가 낮아서 ulimit 등을 조정해 10만 개 이상 열 수 있게 바꿨습니다.
somaxconn, TCP 버퍼 크기 같은 값도 연결 수에 맞게 줄였습니다.

Q. 네트워크는 10Gbps인데 왜 더 못 태우나요?
→ 문제는 대역폭이 아니라, **수만 개 연결이 동시에 깨어나며 생기는 지연(p99 튐)**입니다.
5만은 이 구간을 피해가는 안전한 수치입니다.



두 번째 층은 Redis 클러스터입니다. 6개 노드로 구성하여 실시간 데이터를 처리합니다. 여기서 실제 시청자 수 계산과 중복 제거가 이뤄집니다. Redis를 선택한 이유는 초당 10만 TPS 이상의 처리 능력과 1ms 이내의 응답 속도 때문입니다.

세 번째 층은 MySQL 데이터베이스입니다. MySQL은 실시간 처리가 아닌 안정적인 데이터 저장과 복잡한 분석 쿼리 처리에 집중합니다. 

Master-Slave 구조로 고가용성을 확보하고, 읽기 전용 복제본 3대를 통해 조회 부하를 분산시켰습니다. 실시간 레디스 데이터는 5분마다 배치로 업데이트하여 부하를 분산시킵니다.

읽기와 쓰기를 분리하여 Master는 배치 업데이트만, Slave들은 각각 랭킹 조회, 히스토리 조회, 분석 쿼리를 전담하도록 역할을 나누었습니다.



```
🇰🇷 100만 동시 사용자 (국내)
                           |
                    ┌─────────────┐
                    │   Route53   │ 
                    │ (Health Check)│
                    └─────────────┘
                           |
                    ┌─────────────┐
                    │     ALB     │
                    │Application  │
                    │Load Balancer│
                    └─────────────┘
                           |
                           ▼
💻 WebSocket 서버 클러스터 (AWS Seoul Region)
🔄 Auto Scaling Group (기본 4대 → 최대 20대)
┌─────────────────────────────────────────────────┐
│  WebSocket Server Pool (c5.xlarge)             │
├─────────────┬─────────────┬─────────────────────┤
│   WS-01     │   WS-02     │   WS-03 ~ WS-20     │
│ (5만 연결)   │ (5만 연결)   │   (필요시 확장)      │
│ CPU: 4 Core │ CPU: 4 Core │   서버 스펙:        │
│ RAM: 8GB    │ RAM: 8GB    │   • 5만 동시연결    │
│ 연결당: 8KB │ 연결당: 8KB │   • 메모리 8KB/연결 │
│ 총: 400MB   │ 총: 400MB   │   • 99.9% 가동률    │
└─────────────┴─────────────┴─────────────────────┘
        │             │             │
        └─────────────┼─────────────┘
                      │
                      ▼
📊 Redis 클러스터 (한국 리전 집중)
🚀 Redis Cluster (r5.2xlarge × 6대)
┌─────────────────────────────────────────────────────────┐
│  Master Nodes (3개)          Slave Nodes (3개)         │
├───────────────────────────┬─────────────────────────────┤
│                           │                             │
│  📈 Master-1 (AZ-2a)      │  📈 Slave-1 (AZ-2b)        │
│  • stream_id: 0~33%       │  • 읽기 전용 복제           │
│  • 메모리: 64GB           │  • 장애 시 Master 승격      │
│  • 실시간 시청자 관리     │  • 메모리: 64GB             │
│                           │                             │
│  📈 Master-2 (AZ-2b)      │  📈 Slave-2 (AZ-2c)        │
│  • stream_id: 34~66%      │  • 읽기 전용 복제           │
│  • 메모리: 64GB           │  • 메모리: 64GB             │
│                           │                             │
│  📈 Master-3 (AZ-2c)      │  📈 Slave-3 (AZ-2a)        │
│  • stream_id: 67~100%     │  • 읽기 전용 복제           │
│  • 메모리: 64GB           │  • 메모리: 64GB             │
└───────────────────────────┴─────────────────────────────┘
                      │
                      ▼
🗄️ Aurora MySQL 클러스터 (Multi-AZ 서버리스)
💾 Amazon Aurora MySQL Cluster
┌─────────────────────────────────────────────────────────┐
│                Aurora MySQL Cluster                     │
│  ┌─────────────────────────────────────────────────┐   │
│  │             🔥 Writer Instance                   │   │
│  │           (Primary - AZ-2a)                     │   │
│  │  ┌─────────────────────────────────────────┐   │   │
│  │  │ • db.r5.4xlarge (16 vCPU, 128GB RAM)   │   │   │
│  │  │ • 쓰기 전용 (Write-Only)                │   │   │
│  │  │ • 실시간 데이터 배치 업데이트           │   │   │
│  │  │ • current_viewers 테이블                │   │   │
│  │  │ • viewer_history 테이블                 │   │   │
│  │  │ • Aurora 자동 백업/스냅샷               │   │   │
│  │  └─────────────────────────────────────────┘   │   │
│  └─────────────────────────────────────────────────┘   │
└─────────────────────────────────────────────────────────┘
                      │
                      │ Aurora 복제 (< 10ms)
                      ▼
┌─────────────────────────────────────────────────────────┐
│                  Aurora Reader Instances               │
├─────────────────┬───────────────────┬───────────────────┤
│📖 Reader-1      │📖 Reader-2        │📖 Reader-3        │
│(AZ-2a)          │(AZ-2b)            │(AZ-2c)            │
│                 │                   │                   │
│🎯 랭킹조회       │📈 히스토리 조회    │📊 분석쿼리        │
│• 실시간 시청자순│• 시계열 데이터     │• 통계 리포트      │
│• 인기방송 목록  │• 그래프 데이터     │• 트렌드 분석      │
│• 카테고리별 순위│• 시간대별 비교     │• 대시보드 데이터  │
│• Auto Scaling   │• 방송인별 히스토리 │• 수익 분석        │
│• Connection Pool│• 성장률 계산       │• 예측 모델링      │
│                 │                   │                   │
│db.r5.2xlarge    │db.r5.2xlarge      │db.r5.2xlarge      │
│8 vCPU, 64GB     │8 vCPU, 64GB       │8 vCPU, 64GB       │
└─────────────────┴───────────────────┴───────────────────┘
```

핵심 설계 원칙은 실시간 처리와 안정성을 동시에 확보하는 것이었습니다. WebSocket으로 실시간 연결을 처리하고, Redis로 빠른 카운팅을, Aurora MySQL로 데이터 영속성을 보장하는 3-tier 구조입니다.


## 3. MySQL 한계 발견과 Redis 도입 결정 과정 (3분)

처음에는 Aurora MySQL만으로 해결하려고 했습니다. db.r5.4xlarge 인스턴스면 16 vCPU에 128GB RAM이니까 충분하지 않을까 생각했습니다.

TPS 10,000까지는 평균 응답시간이 15ms 정도로 안정적일 것으로 추정됩니다. 하지만 TPS 15,000에서부터 응답시간이 150ms로 늘어나고 불안정해질 것으로 예상됩니다. 그리고 TPS 20,000에서는 응답시간이 5초 이상으로 늘어나고 에러율이 25%까지 치솟을 것으로 추정됩니다.

그리고 이때 CPU 사용률이 30%밖에 안 될 것으로 예상된다는 점입니다. 16개 코어가 다 놀고 있는데 왜 성능이 안 나올까 분석해보니, 락 경합이 원인일 것으로 판단됩니다.

예를 들어, 인기 방송 하나에 1초 동안 1,000명이 입장을 시도하면 이런 상황이 벌어집니다. 첫 번째 트랜잭션이 "UPDATE current_viewers SET viewer_count = 8501 WHERE stream_id = 12345"를 실행하고 있으면, 나머지 999개 트랜잭션은 모두 대기해야 합니다. InnoDB의 Row-level Lock 때문이죠. 

결국 메모리도 충분하고 CPU도 여유가 있는데, 같은 로우를 수정하는 작업은 순차 처리만 가능하다 보니 병목이 발생한 겁니다. 또한 MySQL은 ACID 특성을 보장하기 위해 트랜잭션 로그를 디스크에 써야 합니다. 아무리 SSD를 사용해도 물리적인 I/O 한계가 발생하게 됩니다.

이 문제를 해결하기 위해 Redis 도입을 결정했습니다.

## 4. Redis 클러스터 설계 상세 (3분)

Redis의 핵심 장점

"메모리 기반 처리와 원자적 연산이 핵심입니다"
Redis는 모든 데이터를 메모리에 저장하므로 디스크 I/O가 없어 초당 10만 TPS 이상 처리가 가능합니다. 평균 응답 시간도 1ms 이내로 실시간 서비스에 최적화되어 있습니다.
가장 중요한 것은 SET 자료구조입니다. 시청자 목록을 SET으로 관리하면 동일한 사용자가 여러 번 추가되어도 자동으로 중복이 제거됩니다. 이를 통해 복잡한 중복 방지 로직 없이도 정확한 시청자 수를 유지할 수 있습니다.

Redis의 원자적 연산도 중요합니다. SADD, SREM, SCARD 명령어는 모두 원자적으로 실행되므로 동시성 문제 없이 안전하게 시청자 수를 관리할 수 있습니다.

요약하자면,

"각각의 강점을 살린 협업 구조입니다"
Redis는 실시간 데이터 처리를 담당합니다. 사용자 입장/퇴장 즉시 반영하고, 실시간 조회수 표시, 중복 방지 등 1ms 이내 응답이 필요한 모든 작업을 처리합니다.

MySQL은 안정적인 데이터 저장과 복잡한 분석을 담당합니다. Redis의 실시간 데이터를 5분마다 배치로 받아서 영구 저장하고, 시간별/일별 통계 분석, 트렌드 분석, 수익 정산 등을 처리합니다.
데이터 흐름은 단방향입니다. 실시간 변경사항은 Redis에서 시작되어 MySQL로 전파됩니다. 이렇게 하면 데이터 일관성을 유지하면서도 각 시스템의 강점을 최대한 활용할 수 있습니다.


Redis 클러스터를 3 Master + 3 Slave로 구성한 이유는 크게 네 가지입니다.

첫 번째는 성능입니다. Redis Cluster 모드에서 데이터를 키 범위로 샤딩하는데, 3개의 마스터가 stream_id 범위를 0~33%, 34~66%, 67~100%로 나눠서 저장합니다. 이렇게 하면 단일 노드에 쓰기 부하가 몰리지 않고, 피크타임 기준 초당 6,600건 이상의 입퇴장 요청을 마스터 3대가 분산 처리할 수 있습니다.

두 번째는 고가용성입니다. 각 마스터에 대응하는 슬레이브가 다른 AZ에 배치되어 있어서, 마스터 장애 시 Redis Sentinel이 해당 슬레이브를 30초 이내에 자동 승격합니다. AZ 단위 장애에도 데이터를 잃지 않고 즉시 페일오버가 가능하죠.

세 번째는 읽기 부하 분산입니다. 실시간 시청자수 조회는 대부분 읽기 요청이므로, 읽기는 슬레이브에서 처리하도록 구성했습니다. 인기 방송의 경우 조회 요청이 초당 수천 건 이상 발생하는데, 슬레이브로 읽기를 분산하면 마스터의 쓰기 성능 저하를 방지할 수 있습니다.

네 번째는 확장성입니다. 향후 트래픽이 늘어나면 마스터-슬레이브 쌍을 추가해서 수평 확장이 가능합니다.

## 5. 테이블 설계와 데이터 흐름 (2분)

"실시간성과 정확성의 균형을 맞췄습니다"

총 8개의 핵심 테이블로 구성했습니다. users, broadcasters, streams, categories는 기본 마스터 데이터로 완전히 정규화했습니다. 반면 current_viewers와 daily_statistics는 성능을 위해 의도적으로 비정규화를 적용했습니다.

가장 중요한 테이블인 current_viewers는 stream_id당 하나의 레코드만 가지며, 실시간 시청자 수와 최고 시청자 수를 저장합니다. 이 테이블은 초당 수천 번 업데이트되므로 단순한 구조로 설계했습니다.

viewer_history 테이블은 5분마다 시청자 수 변화를 기록하는 시계열 데이터입니다. 월별 파티셔닝을 적용하여 12개월치 데이터를 효율적으로 관리합니다.

인덱스 전략

"조회 패턴을 분석하여 최적화했습니다"
가장 중요한 인덱스는 실시간 랭킹 조회용 복합 인덱스입니다. status, viewer_count DESC, started_at DESC 순으로 구성하여 "현재 라이브 중인 방송을 시청자 수 순으로" 조회하는 메인 화면 쿼리를 최적화했습니다.

커버링 인덱스도 적극 활용했습니다. 자주 조회되는 컬럼들을 인덱스에 포함시켜 테이블 접근 없이 인덱스만으로 결과를 반환할 수 있도록 했습니다.

방송인별 히스토리 조회를 위해서는 broadcaster_id와 recorded_at을 조합한 복합 인덱스를 생성했습니다. 카디널리티가 높은 컬럼을 앞에 배치하여 인덱스 효율성을 극대화했습니다.
파티셔닝 전략

"대용량 데이터를 효율적으로 관리하기 위해 시계열 파티셔닝을 적용했습니다"

viewer_history는 월별 Range 파티셔닝을 적용했습니다. 매월 수억 건의 레코드가 쌓이므로 월단위로 분할하여 쿼리 성능을 유지합니다. 12개월이 지난 파티션은 자동으로 삭제하는 프로시저를 구축했습니다.

user_sessions는 일별 파티셔닝을 적용했습니다. GDPR 규정에 따라 30일만 보관하므로 매일 새 파티션을 생성하고 30일 된 파티션을 삭제합니다.
파티션 관리는 완전히 자동화했습니다. 매일 새벽 2시에 실행되는 배치 작업이 새 파티션 생성과 오래된 파티션 삭제를 담당합니다.

정규화 vs 성능 트레이드오프
"비즈니스 요구사항에 따라 선택적으로 적용했습니다"
기본 마스터 데이터는 3NF를 완전히 준수했습니다. 사용자 정보, 방송 정보, 카테고리 정보는 데이터 일관성이 성능보다 중요하기 때문입니다.

반면 실시간 통계 테이블들은 의도적으로 비정규화했습니다. current_viewers 테이블에는 현재 시청자 수, 최고 시청자 수, 총 시청자 수를 모두 저장하여 조회 시 JOIN 없이 한 번에 모든 정보를 가져올 수 있도록 했습니다.

일별 통계 테이블도 완전히 비정규화하여 복잡한 집계 쿼리 없이 바로 통계 데이터를 조회할 수 있도록 설계했습니다.


## 6. 시스템 레벨 성능 최적화 (2분)

시스템 레벨에서는 두 가지 핵심 최적화를 했습니다.

**첫 번째는 파일 디스크립터 한계 해제입니다.**

"WebSocket 연결 1개당 FD 1개가 필요한데, 서버 1대에 5만 연결이면 최소 5만 FD가 필요합니다"

리눅스 기본 ulimit -n은 1024개라서, 이걸 100000으로 늘렸습니다. 로그, 파일, 내부 파이프까지 고려하면 여유분 포함 10만개를 잡는 게 안전해요.

**두 번째는 TCP 소켓 버퍼 최적화입니다.**

"리눅스 기본값 128KB는 대용량 파일 전송용이고, 우리는 작은 메시지만 주고받습니다"

리눅스 기본값은 128KB인데, 연결 1개당 송수신 버퍼 합쳐서 256KB가 커널 메모리에서 고정으로 잡힙니다. 50,000개 연결이면 12.2GB나 되죠.

하지만 실시간 시청자수 집계는 메시지 크기가 매우 작습니다. 

"입장/퇴장 메시지는 보통 50-80바이트, 시청자수 업데이트는 40-60바이트 수준이에요"

그래서 버퍼를 8KB로 줄였습니다. 그러면 50,000 × 16KB = 0.76GB로 16배나 메모리를 절약할 수 있어요.

**"왜 8KB인가요?"**

실시간 시청자수 집계에서 오가는 메시지들을 구체적으로 분석해보면:

**메시지 크기 분석:**
- 입장 메시지: `{"action":"join","stream_id":12345,"user_id":67890}` (약 50-80바이트)
- 퇴장 메시지: `{"action":"leave","stream_id":12345,"user_id":67890}` (약 50-80바이트)  
- 시청자수 업데이트: `{"count":12345,"stream_id":12345}` (약 40-60바이트)
- Heartbeat: `{"hb":1234567890}` (약 20-30바이트)

**4-5KB는 너무 작음:**
- TCP 헤더(20바이트) + IP 헤더(20바이트) + 이더넷 헤더(14바이트) = 54바이트
- 실제 데이터 + 헤더가 4-5KB를 넘을 수 있음
- **재전송과 ACK 빈도 증가로 CPU 부하 발생**

**6-7KB는 경계선:**
- 대부분의 메시지는 처리 가능하지만 **여유분 부족**
- 네트워크 지터나 헤더 오버헤드 고려 시 불안정할 수 있음

**8KB가 최적인 이유:**
- **안전 마진 확보**: 메시지 크기의 10-20배 여유분
- **TCP 윈도우 크기 최적화**: 표준적인 TCP 설정값
- **메모리 효율성**: 128KB 대비 16배 절약하면서도 안정성 확보

MySQL InnoDB 최적화도 했습니다. 

"데이터베이스 성능을 극대화하기 위해 3가지 핵심 설정을 조정했습니다"

첫 번째는 innodb_buffer_pool_size입니다. 이는 MySQL이 메모리에 데이터를 캐시하는 공간인데, 하지만 우리 시스템은 128GB RAM을 사용하므로, RAM의 70%인 90GB로 설정했습니다.

"왜 70%를 사용하나요?"

100%를 사용하면 운영체제와 다른 프로세스가 사용할 메모리가 부족해집니다. 70%는 MySQL이 데이터를 빠르게 읽고 쓸 수 있으면서도 시스템 안정성을 보장하는 최적 비율입니다.

두 번째는 innodb_flush_log_at_trx_commit입니다. 이 설정은 트랜잭션 로그를 언제 디스크에 쓸지 결정합니다.

"3가지 옵션이 있습니다"

```
innodb_flush_log_at_trx_commit 설정 비교
═══════════════════════════════════════

📊 성능 vs 안전성

값 = 0  [🚀 최고 성능]
트랜잭션 → MySQL 메모리 (1초마다 디스크 쓰기)
위험도: ⚡⚡⚡ (MySQL 크래시 시 최대 1초 손실)
성능:   🚀🚀🚀🚀🚀

값 = 1  [🛡️ 최고 안전성]  
트랜잭션 → 즉시 디스크 쓰기 + fsync()
위험도: ✅ (데이터 손실 없음)
성능:   🚀

값 = 2  [⚖️ 균형점] ← 우리 선택
트랜잭션 → OS 버퍼 (1초마다 디스크 쓰기)
위험도: ⚡ (OS 크래시 시에만 손실)
성능:   🚀🚀🚀🚀
```

우리는 2를 선택했습니다. "왜 2를 선택했나요?"

우리 시스템은 실시간 시청자수 집계가 핵심이므로, 1초 정도의 데이터 손실은 감수할 수 있습니다. 대신 성능이 1보다 2-3배 빨라져서 초당 수천 건의 업데이트를 안정적으로 처리할 수 있습니다.

세 번째는 innodb_io_capacity입니다. 이는 MySQL이 디스크 I/O를 얼마나 빠르게 처리할 수 있는지 설정합니다.

"기본값은 200인데, 왜 2000으로 설정했나요?"

기본값 200은 HDD 기준으로 설정되어 있습니다. 하지만 우리는 SSD를 사용하므로, SSD의 실제 성능에 맞춰 2000으로 설정했습니다. 이렇게 하면 배치 업데이트 시 디스크 쓰기 속도가 10배 빨라져서, 5분마다 실행되는 대량 데이터 동기화가 훨씬 빠르게 완료됩니다.

innodb_io_capacity 설정 원리

       1. 하드웨어 실제 성능
              SSD IOPS (Input/Output Operations Per Second) 측정
              우리 시스템의 SSD: 대략 2000-3000 IOPS
              안전 마진을 고려해 2000 설정

       2. 다른 프로세스와의 공존
              MySQL만 SSD를 사용하는 게 아님
              OS, 로그 파일, 다른 애플리케이션도 I/O 사용
              전체 용량의 60-70% 정도만 MySQL에 할당

       3. CPU와 메모리 리소스 균형
              I/O가 너무 빠르면 CPU 병목 발생 가능
              메모리 버퍼 처리 속도와 균형 맞춰야 함


## 7. 장애 복구 전략 (1분)

장애 복구는 **4가지 핵심 시나리오**로 대응합니다.

**첫 번째는 WebSocket 서버 장애입니다.**

"20대 서버 중 하나가 갑자기 꺼지면 5만 명의 사용자가 연결이 끊어집니다"

다른 서버들이 30초마다 헬스체크를 수행하여 장애를 감지하면, Redis에서 해당 서버가 관리하던 사용자들을 일괄 제거합니다. 사용자들은 브라우저에서 자동으로 다른 서버에 재연결되며, 전체 복구 시간은 1-2분입니다.

**두 번째는 Redis 클러스터 장애입니다.**

"Redis가 완전히 맛이 가면 어떻게 하나요?"

Redis 클러스터에 문제가 생기면 3단계로 대응합니다.

**1단계 자동 복구**: Redis Sentinel이 30초~2분 내에 장애 마스터를 감지하고 슬레이브를 자동 승격합니다.

**2단계 백업 데이터**: 5분마다 Redis 데이터를 백업해둔 PostgreSQL을 사용하여 5분 전 데이터라도 표시합니다.

**3단계 기능 제한**: 최악의 경우 "집계 중" 메시지를 표시하되, 채팅이나 좋아요 같은 다른 기능은 정상 작동합니다.

**세 번째는 Aurora Writer 장애입니다.**

Aurora 특화 복구 메커니즘으로 15초 내에 Reader 중 하나를 Writer로 자동 승격시킵니다. 6-way 스토리지 복제로 데이터 무손실을 보장하고, 공유 스토리지라 승격 시간이 최소화됩니다.

**네 번째는 데이터 정합성 검증과 자동 복구입니다.**

"불일치 정도에 따라 3단계로 나누어 대응합니다"

**1단계 경량 복구 (5-10% 불일치)**는 "유령 사용자"들을 조용히 제거하는 방식입니다. 이 과정은 30초~2분 내에 완료되어 사용자들이 거의 느끼지 못합니다.

**2단계 중간 복구 (10-30% 불일치)**는 해당 방송의 시청자수 업데이트를 일시 중단하고 "동기화 중" 메시지를 표시합니다. 그 다음 WebSocket 기준으로 Redis 데이터를 완전히 재구성하는데, 이 과정은 2-10분 정도 소요됩니다.

**3단계 전면 복구 (30% 이상 불일치)**는 시청자수 기능을 일시적으로 비활성화합니다. 모든 WebSocket 서버의 실제 연결 상태를 수집한 후, Redis 데이터를 완전히 새로 구성하는데 10-30분 정도 걸립니다.

**핵심은 사용자 경험을 최대한 보호하는 것입니다**

시청자수만 잠시 안 보일 뿐 채팅이나 좋아요 같은 다른 기능은 정상적으로 작동합니다. 자동 복구 시스템으로 수동 개입을 최소화하고, 단계별 대응으로 서비스 중단 시간을 최소화합니다.


8. 확장성 설계

"선형 확장이 가능한 구조로 설계했습니다"
현재 100만 동시접속을 20대 서버로 처리하므로, 500만이 되면 100대 서버로 선형 확장이 가능합니다. 각 서버는 완전히 독립적으로 동작하므로 새 서버 추가만으로 용량을 늘릴 수 있습니다.

지역별 분산도 고려했습니다. 글로벌 서비스로 확장할 때는 아시아, 북미, 유럽에 각각 독립적인 클러스터를 구축하여 지연시간을 최소화할 수 있습니다.

데이터베이스는 샤딩 전략으로 확장합니다. stream_id를 기준으로 해시 분산하여 여러 DB에 분산 저장하고,  애플리케이션 레벨에서 적절한 샤드로 라우팅합니다.

자동 스케일링도 구현했습니다. 서버별 연결 수가 45,000개를 넘으면 자동으로 새 서버를 추가하고, 트래픽이 줄어들면 점진적으로 서버를 제거합니다.
