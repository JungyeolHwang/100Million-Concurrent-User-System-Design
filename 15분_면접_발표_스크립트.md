# 100만 동시접속 실시간 시청자수 시스템 - 15분 면접 발표 스크립트

## 1. 시스템 아키텍처 소개 (2분)

안녕하세요. 오늘은 100만 동시접속 실시간 시청자수 시스템 설계에 대해 발표하겠습니다.

먼저 전체 아키텍처를 보시면, 크게 3개 레이어로 구성되어 있습니다.

```
🇰🇷 100만 동시 사용자 (국내)
                           |
                    ┌─────────────┐
                    │   Route53   │ 
                    │ (Health Check)│
                    └─────────────┘
                           |
                    ┌─────────────┐
                    │     ALB     │
                    │Application  │
                    │Load Balancer│
                    └─────────────┘
                           |
                           ▼
💻 WebSocket 서버 클러스터 (AWS Seoul Region)
🔄 Auto Scaling Group (기본 4대 → 최대 20대)
┌─────────────────────────────────────────────────┐
│  WebSocket Server Pool (c5.2xlarge)            │
├─────────────┬─────────────┬─────────────────────┤
│   WS-01     │   WS-02     │   WS-03 ~ WS-20     │
│ (5만 연결)   │ (5만 연결)   │   (필요시 확장)      │
│ CPU: 8 Core │ CPU: 8 Core │   서버 스펙:        │
│ RAM: 16GB   │ RAM: 16GB   │   • 5만 동시연결    │
│ 연결당: 4KB │ 연결당: 4KB │   • 메모리 4KB/연결 │
│ 총: 200MB   │ 총: 200MB   │   • 99.9% 가동률    │
└─────────────┴─────────────┴─────────────────────┘
        │             │             │
        └─────────────┼─────────────┘
                      │
                      ▼
📊 Redis 클러스터 (한국 리전 집중)
🚀 Redis Cluster (r5.2xlarge × 6대)
┌─────────────────────────────────────────────────────────┐
│  Master Nodes (3개)          Slave Nodes (3개)         │
├───────────────────────────┬─────────────────────────────┤
│                           │                             │
│  📈 Master-1 (AZ-2a)      │  📈 Slave-1 (AZ-2b)        │
│  • stream_id: 0~33%       │  • 읽기 전용 복제           │
│  • 메모리: 64GB           │  • 장애 시 Master 승격      │
│  • 실시간 시청자 관리     │  • 메모리: 64GB             │
│                           │                             │
│  📈 Master-2 (AZ-2b)      │  📈 Slave-2 (AZ-2c)        │
│  • stream_id: 34~66%      │  • 읽기 전용 복제           │
│  • 메모리: 64GB           │  • 메모리: 64GB             │
│                           │                             │
│  📈 Master-3 (AZ-2c)      │  📈 Slave-3 (AZ-2a)        │
│  • stream_id: 67~100%     │  • 읽기 전용 복제           │
│  • 메모리: 64GB           │  • 메모리: 64GB             │
└───────────────────────────┴─────────────────────────────┘
                      │
                      ▼
🗄️ Aurora MySQL 클러스터 (Multi-AZ 서버리스)
💾 Amazon Aurora MySQL Cluster
┌─────────────────────────────────────────────────────────┐
│                Aurora MySQL Cluster                     │
│  ┌─────────────────────────────────────────────────┐   │
│  │             🔥 Writer Instance                   │   │
│  │           (Primary - AZ-2a)                     │   │
│  │  ┌─────────────────────────────────────────┐   │   │
│  │  │ • db.r5.4xlarge (16 vCPU, 128GB RAM)   │   │   │
│  │  │ • 쓰기 전용 (Write-Only)                │   │   │
│  │  │ • 실시간 데이터 배치 업데이트           │   │   │
│  │  │ • current_viewers 테이블                │   │   │
│  │  │ • viewer_history 테이블                 │   │   │
│  │  │ • Aurora 자동 백업/스냅샷               │   │   │
│  │  └─────────────────────────────────────────┘   │   │
│  └─────────────────────────────────────────────────┘   │
└─────────────────────────────────────────────────────────┘
                      │
                      │ Aurora 복제 (< 10ms)
                      ▼
┌─────────────────────────────────────────────────────────┐
│                  Aurora Reader Instances               │
├─────────────────┬───────────────────┬───────────────────┤
│📖 Reader-1      │📖 Reader-2        │📖 Reader-3        │
│(AZ-2a)          │(AZ-2b)            │(AZ-2c)            │
│                 │                   │                   │
│🎯 랭킹조회       │📈 히스토리 조회    │📊 분석쿼리        │
│• 실시간 시청자순│• 시계열 데이터     │• 통계 리포트      │
│• 인기방송 목록  │• 그래프 데이터     │• 트렌드 분석      │
│• 카테고리별 순위│• 시간대별 비교     │• 대시보드 데이터  │
│• Auto Scaling   │• 방송인별 히스토리 │• 수익 분석        │
│• Connection Pool│• 성장률 계산       │• 예측 모델링      │
│                 │                   │                   │
│db.r5.2xlarge    │db.r5.2xlarge      │db.r5.2xlarge      │
│8 vCPU, 64GB     │8 vCPU, 64GB       │8 vCPU, 64GB       │
└─────────────────┴───────────────────┴───────────────────┘
```

핵심 설계 원칙은 실시간 처리와 안정성을 동시에 확보하는 것이었습니다. WebSocket으로 실시간 연결을 처리하고, Redis로 빠른 카운팅을, Aurora MySQL로 데이터 영속성을 보장하는 3-tier 구조입니다.

## 2. 비즈니스 요구사항 분석과 TPS 산정 (2분)

설계를 시작하기 전에 가장 먼저 한 일은 정확한 TPS 산정이었습니다. 

라이브 스트리밍 플랫폼에서 평균 시청시간은 보통 15분 정도입니다. 이를 기반으로 계산해보면, 100만 동시접속자가 있을 때 시간당 회전율은 4회가 됩니다. 그러면 시간당 총 800만번의 입장과 퇴장이 발생하게 되죠.

이를 초당으로 환산하면 기본적으로 2,222건의 TPS가 나옵니다. 하지만 여기서 중요한 건 트래픽 집중 현상이었습니다. 

실제 라이브 스트리밍에서는 인기 방송에 트래픽이 몰리는 패턴이 있어서, 이를 고려해 3배 여유분을 둬서 피크타임 6,600 TPS로 설계했습니다.

그런데 여기서 더 큰 문제를 발견했습니다. 유명 스트리머가 갑자기 방송을 시작하면 5분 안에 10만명이 몰리는 상황이 발생합니다. 이때는 TPS가 25,000까지 급증하게 되는데, 이것이 바로 MySQL 단독으로는 처리할 수 없는 이유였습니다.

## 3. MySQL 한계 발견과 Redis 도입 결정 과정 (3분)

처음에는 당연히 Aurora MySQL만으로 해결하려고 했습니다. db.r5.4xlarge 인스턴스면 16 vCPU에 128GB RAM이니까 충분할 거라고 생각했거든요.

하지만 직접 부하 테스트를 해본 결과는 충격적이었습니다.

TPS 10,000까지는 평균 응답시간이 15ms로 안정적이었어요. 하지만 TPS 15,000에서부터 응답시간이 150ms로 늘어나고 불안정해지기 시작했습니다. 그리고 TPS 20,000에서는 응답시간이 5초 이상으로 늘어나고 에러율이 25%까지 치솟았어요.

더 놀라운 건 이때 CPU 사용률이 30%밖에 안 됐다는 겁니다. 16개 코어가 다 놀고 있는데 왜 성능이 안 나올까 분석해보니, 락 경합이 원인이었습니다.

예를 들어, 인기 방송 하나에 1초 동안 1,000명이 입장을 시도하면 이런 상황이 벌어집니다. 첫 번째 트랜잭션이 "UPDATE current_viewers SET viewer_count = 8501 WHERE stream_id = 12345"를 실행하고 있으면, 나머지 999개 트랜잭션은 모두 대기해야 합니다. InnoDB의 Row-level Lock 때문이죠.

결국 메모리도 충분하고 CPU도 여유가 있는데, 같은 로우를 수정하는 작업은 순차 처리만 가능하다 보니 병목이 발생한 겁니다.

이 문제를 해결하기 위해 Redis 도입을 결정했습니다. Redis는 메모리 기반이라 빠르고, INCR/DECR 같은 원자적 연산으로 락 경합 없이 카운팅을 처리할 수 있거든요. 그리고 샤딩을 통해 부하를 분산시킬 수도 있고요.

## 4. Redis 클러스터 설계 상세 (3분)

Redis 클러스터를 3 Master + 3 Slave로 구성한 이유는 크게 네 가지입니다.

첫 번째는 성능입니다. Redis Cluster 모드에서 데이터를 키 범위로 샤딩하는데, 3개의 마스터가 stream_id 범위를 0~33%, 34~66%, 67~100%로 나눠서 저장합니다. 이렇게 하면 단일 노드에 쓰기 부하가 몰리지 않고, 피크타임 기준 초당 6,600건 이상의 입퇴장 요청을 마스터 3대가 분산 처리할 수 있습니다.

두 번째는 고가용성입니다. 각 마스터에 대응하는 슬레이브가 다른 AZ에 배치되어 있어서, 마스터 장애 시 Redis Sentinel이 해당 슬레이브를 30초 이내에 자동 승격합니다. AZ 단위 장애에도 데이터를 잃지 않고 즉시 페일오버가 가능하죠.

세 번째는 읽기 부하 분산입니다. 실시간 시청자수 조회는 대부분 읽기 요청이므로, 읽기는 슬레이브에서 처리하도록 구성했습니다. 인기 방송의 경우 조회 요청이 초당 수천 건 이상 발생하는데, 슬레이브로 읽기를 분산하면 마스터의 쓰기 성능 저하를 방지할 수 있습니다.

네 번째는 장애 도메인 분리와 확장성입니다. 3개 AZ에 마스터-슬레이브를 번갈아 배치해서, 한 AZ 장애 시에도 전체 클러스터의 2/3 이상이 정상 동작하고, 향후 트래픽이 늘어나면 마스터-슬레이브 쌍을 추가해서 수평 확장이 가능합니다.

데이터 정확성도 신경 썼습니다. 실시간 동접은 live:users:{stream_id}를 Set 구조로 관리해서 중복을 방지하고, 멀티탭 구분을 위해 live:tabs:{stream_id}:{user_id}로 세션을 추적합니다. 그리고 비정상 종료에 대비해서 hb:{session}으로 TTL 60초의 Heartbeat를 운영합니다.

## 5. 테이블 설계와 데이터 흐름 (2분)

데이터베이스는 8개 테이블로 설계했는데, 핵심은 기능별 책임 분리입니다.

users, broadcasters, categories, streams는 기본 마스터 데이터를 관리합니다. 변경이 적고 조회 중심이라 트랜잭션 부하가 적어요.

current_viewers는 Redis와 동기화되는 실시간 카운트 전용 테이블입니다. 고빈도 업데이트를 처리하기 위해 별도 테이블로 분리했고, JOIN 없는 단순 PK 업서트 형태로 비정규화했습니다.

viewer_history는 5분 단위 스냅샷 저장용으로, current_viewers의 이력을 장기 보관하고 분석할 수 있도록 설계했습니다. 월별 파티셔닝을 적용해서 대용량 데이터 관리가 쉽습니다.

user_sessions는 개별 시청자의 접속 세션 정보를 관리하고, GDPR 등 보존 기간 관리를 위해 일 단위 파티셔닝을 적용했습니다.

daily_statistics는 집계 결과를 그대로 저장하는 비정규화 테이블로, OLAP 성격이라 OLTP 테이블과 분리해서 조회 부하가 실시간 데이터에 영향을 주지 않도록 했습니다.

핵심은 실시간 처리와 분석 쿼리의 완전한 분리입니다. 이렇게 하면 트랜잭션 락 경합을 피하고 Aurora Writer 부하를 최소화할 수 있어요.

## 6. 시스템 레벨 성능 최적화 (2분)

시스템 레벨에서는 두 가지 핵심 최적화를 했습니다.

먼저 파일 디스크립터 한계 해제입니다. WebSocket 연결 1개당 FD 1개가 필요한데, 서버 1대에 5만 연결이면 최소 5만 FD가 필요합니다. 로그, 파일, 내부 파이프까지 고려하면 여유분 포함 10만개를 잡는 게 안전해요. 리눅스 기본 ulimit -n은 1024개라서, 이걸 100000으로 늘렸습니다.

두 번째는 TCP 소켓 버퍼 최적화입니다. 리눅스 기본값은 128KB인데, 연결 1개당 송수신 버퍼 합쳐서 256KB가 커널 메모리에서 고정으로 잡힙니다. 50,000개 연결이면 12.2GB나 되죠.

하지만 실시간 시청자수 집계는 메시지 크기가 매우 작습니다. 보통 수십에서 수백 바이트 수준이에요. 그래서 버퍼를 8KB로 줄였습니다. 그러면 50,000 × 16KB = 0.76GB로 16배나 메모리를 절약할 수 있어요.

MySQL InnoDB 최적화도 했습니다. innodb_buffer_pool_size는 RAM의 70%인 90GB로 설정하고, 배치 업데이트가 많다 보니 innodb_flush_log_at_trx_commit를 2로 설정해서 성능을 우선했습니다. 그리고 SSD 환경에 맞춰 innodb_io_capacity를 2000으로 설정했어요.

## 7. 장애 복구 전략 (1분)

장애 복구는 3단계 방어선으로 구성했습니다.

첫 번째는 Redis 장애 대응입니다. 마스터 장애 시 Redis Sentinel이 30초 내에 슬레이브를 자동 승격시키고, AZ 단위 장애에도 다른 AZ의 노드들이 서비스를 계속 유지합니다.

두 번째는 Aurora Writer 장애 대응입니다. Aurora 특화 복구 메커니즘으로 15초 내에 Reader 중 하나를 Writer로 자동 승격시킵니다. 6-way 스토리지 복제로 데이터 무손실을 보장하고, 공유 스토리지라 승격 시간이 최소화됩니다.

세 번째는 정합성 검증과 자동 복구입니다. 솔직히 말씀드리면, 저는 Redis를 실제 운영 환경에서 사용해본 경험이 없어서 이 부분은 이론적 추정과 개념적 설계로 설명드리겠습니다.

분산 시스템에서는 필연적으로 데이터 불일치가 발생할 수 있습니다. 네트워크 플랩, Redis 일시적 장애, WebSocket close 이벤트 누락 등으로 실제 연결 상태와 Redis 카운트가 어긋날 수 있거든요.

이런 문제를 해결하기 위해 개념적으로 3단계 복구 프로세스를 설계했습니다.

첫째, 각 WebSocket 서버가 주기적으로 자신의 실제 연결 수를 Redis에 보고하는 모니터링 체계입니다. 

둘째, 시스템이 모든 서버의 실제 연결 수 합계와 Redis의 카운트를 비교해서 일정 임계값 이상 차이가 나면 불일치를 감지하는 메커니즘입니다.

셋째, 불일치 감지 시 WebSocket 서버의 실제 연결 상태를 진실 원장으로 보고, 이를 기준으로 Redis 데이터를 재동기화하는 복구 프로세스입니다.

또한 비정상 종료된 연결을 감지하기 위한 Heartbeat나 TTL 같은 메커니즘도 필요할 것으로 생각됩니다. 

실제로는 Redis 운영 경험이 있는 시니어 개발자나 Redis 커뮤니티의 베스트 프랙티스를 참고해서 더 구체적인 구현 방법을 결정해야 할 것 같습니다.

무엇보다 이론이 아닌 실측 데이터로 설계를 검증했다는 점이 가장 자랑스럽습니다. 질문 있으시면 언제든 말씀해주세요.
