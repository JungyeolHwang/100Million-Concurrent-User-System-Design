# 100만 동시접속 실시간 시청자수 시스템 - 15분 면접 발표 스크립트


안녕하세요. 오늘은 100만 동시접속 실시간 시청자수 시스템 설계에 대해 발표하겠습니다.

## 1. 비즈니스 요구사항 분석과 TPS 산정 (2분)

설계를 시작하기 전에 가장 먼저 한 일은 정확한 TPS 산정이었습니다. 

라이브 스트리밍 플랫폼에서 평균 시청시간은 보통 15분 정도입니다. 이를 기반으로 계산해보면, 100만 동시접속자가 있을 때 시간당 회전율은 4회가 됩니다. 그러면 시간당 총 800만번의 입장과 퇴장이 발생하게 되죠.

이를 초당으로 환산하면 기본적으로 2,222건의 TPS가 나옵니다. 하지만 여기서 중요한 건 트래픽 집중 현상이었습니다. 

실제 라이브 스트리밍에서는 인기 방송에 트래픽이 몰리는 패턴이 있어서, 이를 고려해 3배 여유분을 둬서 피크타임 6,600 TPS로 설계했습니다.

그런데 여기서 더 큰 문제를 발견했습니다. 유명 스트리머가 갑자기 방송을 시작하면 5분 안에 10만명이 몰리는 상황이 발생합니다. 이때는 TPS가 25,000까지 급증하게 되는데, 


## 2. 시스템 아키텍처 소개 (2분)


먼저 전체 아키텍처를 보시면, 크게 3개 레이어로 구성되어 있습니다.

"100만 동시접속을 처리하기 위해 3층 구조로 설계했습니다"

첫 번째 층은 WebSocket 서버 풀입니다. 20대의 서버가 각각 5만 연결을 담당하여 총 100만 연결을 처리합니다. 로드밸런서가 사용자를 적절히 분산시키되, 같은 사용자는 같은 서버로 연결되도록 Session Affinity를 적용했습니다.

두 번째 층은 Redis 클러스터입니다. 6개 노드로 구성하여 실시간 데이터를 처리합니다. 여기서 실제 시청자 수 계산과 중복 제거가 이뤄집니다. Redis를 선택한 이유는 초당 10만 TPS 이상의 처리 능력과 1ms 이내의 응답 속도 때문입니다.

세 번째 층은 MySQL 데이터베이스입니다. MySQL은 실시간 처리가 아닌 안정적인 데이터 저장과 복잡한 분석 쿼리 처리에 집중합니다. 

Master-Slave 구조로 고가용성을 확보하고, 읽기 전용 복제본 3대를 통해 조회 부하를 분산시켰습니다. 실시간 레디스 데이터는 5분마다 배치로 업데이트하여 부하를 분산시킵니다.

읽기와 쓰기를 분리하여 Master는 배치 업데이트만, Slave들은 각각 랭킹 조회, 히스토리 조회, 분석 쿼리를 전담하도록 역할을 나누었습니다.



```
🇰🇷 100만 동시 사용자 (국내)
                           |
                    ┌─────────────┐
                    │   Route53   │ 
                    │ (Health Check)│
                    └─────────────┘
                           |
                    ┌─────────────┐
                    │     ALB     │
                    │Application  │
                    │Load Balancer│
                    └─────────────┘
                           |
                           ▼
💻 WebSocket 서버 클러스터 (AWS Seoul Region)
🔄 Auto Scaling Group (기본 4대 → 최대 20대)
┌─────────────────────────────────────────────────┐
│  WebSocket Server Pool (c5.2xlarge)            │
├─────────────┬─────────────┬─────────────────────┤
│   WS-01     │   WS-02     │   WS-03 ~ WS-20     │
│ (5만 연결)   │ (5만 연결)   │   (필요시 확장)      │
│ CPU: 8 Core │ CPU: 8 Core │   서버 스펙:        │
│ RAM: 16GB   │ RAM: 16GB   │   • 5만 동시연결    │
│ 연결당: 4KB │ 연결당: 4KB │   • 메모리 4KB/연결 │
│ 총: 200MB   │ 총: 200MB   │   • 99.9% 가동률    │
└─────────────┴─────────────┴─────────────────────┘
        │             │             │
        └─────────────┼─────────────┘
                      │
                      ▼
📊 Redis 클러스터 (한국 리전 집중)
🚀 Redis Cluster (r5.2xlarge × 6대)
┌─────────────────────────────────────────────────────────┐
│  Master Nodes (3개)          Slave Nodes (3개)         │
├───────────────────────────┬─────────────────────────────┤
│                           │                             │
│  📈 Master-1 (AZ-2a)      │  📈 Slave-1 (AZ-2b)        │
│  • stream_id: 0~33%       │  • 읽기 전용 복제           │
│  • 메모리: 64GB           │  • 장애 시 Master 승격      │
│  • 실시간 시청자 관리     │  • 메모리: 64GB             │
│                           │                             │
│  📈 Master-2 (AZ-2b)      │  📈 Slave-2 (AZ-2c)        │
│  • stream_id: 34~66%      │  • 읽기 전용 복제           │
│  • 메모리: 64GB           │  • 메모리: 64GB             │
│                           │                             │
│  📈 Master-3 (AZ-2c)      │  📈 Slave-3 (AZ-2a)        │
│  • stream_id: 67~100%     │  • 읽기 전용 복제           │
│  • 메모리: 64GB           │  • 메모리: 64GB             │
└───────────────────────────┴─────────────────────────────┘
                      │
                      ▼
🗄️ Aurora MySQL 클러스터 (Multi-AZ 서버리스)
💾 Amazon Aurora MySQL Cluster
┌─────────────────────────────────────────────────────────┐
│                Aurora MySQL Cluster                     │
│  ┌─────────────────────────────────────────────────┐   │
│  │             🔥 Writer Instance                   │   │
│  │           (Primary - AZ-2a)                     │   │
│  │  ┌─────────────────────────────────────────┐   │   │
│  │  │ • db.r5.4xlarge (16 vCPU, 128GB RAM)   │   │   │
│  │  │ • 쓰기 전용 (Write-Only)                │   │   │
│  │  │ • 실시간 데이터 배치 업데이트           │   │   │
│  │  │ • current_viewers 테이블                │   │   │
│  │  │ • viewer_history 테이블                 │   │   │
│  │  │ • Aurora 자동 백업/스냅샷               │   │   │
│  │  └─────────────────────────────────────────┘   │   │
│  └─────────────────────────────────────────────────┘   │
└─────────────────────────────────────────────────────────┘
                      │
                      │ Aurora 복제 (< 10ms)
                      ▼
┌─────────────────────────────────────────────────────────┐
│                  Aurora Reader Instances               │
├─────────────────┬───────────────────┬───────────────────┤
│📖 Reader-1      │📖 Reader-2        │📖 Reader-3        │
│(AZ-2a)          │(AZ-2b)            │(AZ-2c)            │
│                 │                   │                   │
│🎯 랭킹조회       │📈 히스토리 조회    │📊 분석쿼리        │
│• 실시간 시청자순│• 시계열 데이터     │• 통계 리포트      │
│• 인기방송 목록  │• 그래프 데이터     │• 트렌드 분석      │
│• 카테고리별 순위│• 시간대별 비교     │• 대시보드 데이터  │
│• Auto Scaling   │• 방송인별 히스토리 │• 수익 분석        │
│• Connection Pool│• 성장률 계산       │• 예측 모델링      │
│                 │                   │                   │
│db.r5.2xlarge    │db.r5.2xlarge      │db.r5.2xlarge      │
│8 vCPU, 64GB     │8 vCPU, 64GB       │8 vCPU, 64GB       │
└─────────────────┴───────────────────┴───────────────────┘
```

핵심 설계 원칙은 실시간 처리와 안정성을 동시에 확보하는 것이었습니다. WebSocket으로 실시간 연결을 처리하고, Redis로 빠른 카운팅을, Aurora MySQL로 데이터 영속성을 보장하는 3-tier 구조입니다.


## 3. MySQL 한계 발견과 Redis 도입 결정 과정 (3분)

처음에는 Aurora MySQL만으로 해결하려고 했습니다. db.r5.4xlarge 인스턴스면 16 vCPU에 128GB RAM이니까 충분하지 않을까 생각했습니다.

TPS 10,000까지는 평균 응답시간이 15ms 정도로 안정적일 것으로 추정됩니다. 하지만 TPS 15,000에서부터 응답시간이 150ms로 늘어나고 불안정해질 것으로 예상됩니다. 그리고 TPS 20,000에서는 응답시간이 5초 이상으로 늘어나고 에러율이 25%까지 치솟을 것으로 추정됩니다.

그리고 이때 CPU 사용률이 30%밖에 안 될 것으로 예상된다는 점입니다. 16개 코어가 다 놀고 있는데 왜 성능이 안 나올까 분석해보니, 락 경합이 원인일 것으로 판단됩니다.

예를 들어, 인기 방송 하나에 1초 동안 1,000명이 입장을 시도하면 이런 상황이 벌어집니다. 첫 번째 트랜잭션이 "UPDATE current_viewers SET viewer_count = 8501 WHERE stream_id = 12345"를 실행하고 있으면, 나머지 999개 트랜잭션은 모두 대기해야 합니다. InnoDB의 Row-level Lock 때문이죠. 

결국 메모리도 충분하고 CPU도 여유가 있는데, 같은 로우를 수정하는 작업은 순차 처리만 가능하다 보니 병목이 발생한 겁니다. 또한 MySQL은 ACID 특성을 보장하기 위해 트랜잭션 로그를 디스크에 써야 합니다. 아무리 SSD를 사용해도 물리적인 I/O 한계가 발생하게 됩니다.

이 문제를 해결하기 위해 Redis 도입을 결정했습니다.

## 4. Redis 클러스터 설계 상세 (3분)

Redis의 핵심 장점

"메모리 기반 처리와 원자적 연산이 핵심입니다"
Redis는 모든 데이터를 메모리에 저장하므로 디스크 I/O가 없어 초당 10만 TPS 이상 처리가 가능합니다. 평균 응답 시간도 1ms 이내로 실시간 서비스에 최적화되어 있습니다.
가장 중요한 것은 SET 자료구조입니다. 시청자 목록을 SET으로 관리하면 동일한 사용자가 여러 번 추가되어도 자동으로 중복이 제거됩니다. 이를 통해 복잡한 중복 방지 로직 없이도 정확한 시청자 수를 유지할 수 있습니다.

Redis의 원자적 연산도 중요합니다. SADD, SREM, SCARD 명령어는 모두 원자적으로 실행되므로 동시성 문제 없이 안전하게 시청자 수를 관리할 수 있습니다.

요약하자면,

"각각의 강점을 살린 협업 구조입니다"
Redis는 실시간 데이터 처리를 담당합니다. 사용자 입장/퇴장 즉시 반영하고, 실시간 조회수 표시, 중복 방지 등 1ms 이내 응답이 필요한 모든 작업을 처리합니다.

MySQL은 안정적인 데이터 저장과 복잡한 분석을 담당합니다. Redis의 실시간 데이터를 5분마다 배치로 받아서 영구 저장하고, 시간별/일별 통계 분석, 트렌드 분석, 수익 정산 등을 처리합니다.
데이터 흐름은 단방향입니다. 실시간 변경사항은 Redis에서 시작되어 MySQL로 전파됩니다. 이렇게 하면 데이터 일관성을 유지하면서도 각 시스템의 강점을 최대한 활용할 수 있습니다.


Redis 클러스터를 3 Master + 3 Slave로 구성한 이유는 크게 네 가지입니다.

첫 번째는 성능입니다. Redis Cluster 모드에서 데이터를 키 범위로 샤딩하는데, 3개의 마스터가 stream_id 범위를 0~33%, 34~66%, 67~100%로 나눠서 저장합니다. 이렇게 하면 단일 노드에 쓰기 부하가 몰리지 않고, 피크타임 기준 초당 6,600건 이상의 입퇴장 요청을 마스터 3대가 분산 처리할 수 있습니다.

두 번째는 고가용성입니다. 각 마스터에 대응하는 슬레이브가 다른 AZ에 배치되어 있어서, 마스터 장애 시 Redis Sentinel이 해당 슬레이브를 30초 이내에 자동 승격합니다. AZ 단위 장애에도 데이터를 잃지 않고 즉시 페일오버가 가능하죠.

세 번째는 읽기 부하 분산입니다. 실시간 시청자수 조회는 대부분 읽기 요청이므로, 읽기는 슬레이브에서 처리하도록 구성했습니다. 인기 방송의 경우 조회 요청이 초당 수천 건 이상 발생하는데, 슬레이브로 읽기를 분산하면 마스터의 쓰기 성능 저하를 방지할 수 있습니다.

네 번째는 확장성입니다. 향후 트래픽이 늘어나면 마스터-슬레이브 쌍을 추가해서 수평 확장이 가능합니다.

## 5. 테이블 설계와 데이터 흐름 (2분)

"실시간성과 정확성의 균형을 맞췄습니다"

총 8개의 핵심 테이블로 구성했습니다. users, broadcasters, streams, categories는 기본 마스터 데이터로 완전히 정규화했습니다. 반면 current_viewers와 daily_statistics는 성능을 위해 의도적으로 비정규화를 적용했습니다.

가장 중요한 테이블인 current_viewers는 stream_id당 하나의 레코드만 가지며, 실시간 시청자 수와 최고 시청자 수를 저장합니다. 이 테이블은 초당 수천 번 업데이트되므로 단순한 구조로 설계했습니다.

viewer_history 테이블은 5분마다 시청자 수 변화를 기록하는 시계열 데이터입니다. 월별 파티셔닝을 적용하여 12개월치 데이터를 효율적으로 관리합니다.

인덱스 전략

"조회 패턴을 분석하여 최적화했습니다"
가장 중요한 인덱스는 실시간 랭킹 조회용 복합 인덱스입니다. status, viewer_count DESC, started_at DESC 순으로 구성하여 "현재 라이브 중인 방송을 시청자 수 순으로" 조회하는 메인 화면 쿼리를 최적화했습니다.

커버링 인덱스도 적극 활용했습니다. 자주 조회되는 컬럼들을 인덱스에 포함시켜 테이블 접근 없이 인덱스만으로 결과를 반환할 수 있도록 했습니다.

방송인별 히스토리 조회를 위해서는 broadcaster_id와 recorded_at을 조합한 복합 인덱스를 생성했습니다. 카디널리티가 높은 컬럼을 앞에 배치하여 인덱스 효율성을 극대화했습니다.
파티셔닝 전략

"대용량 데이터를 효율적으로 관리하기 위해 시계열 파티셔닝을 적용했습니다"

viewer_history는 월별 Range 파티셔닝을 적용했습니다. 매월 수억 건의 레코드가 쌓이므로 월단위로 분할하여 쿼리 성능을 유지합니다. 12개월이 지난 파티션은 자동으로 삭제하는 프로시저를 구축했습니다.

user_sessions는 일별 파티셔닝을 적용했습니다. GDPR 규정에 따라 30일만 보관하므로 매일 새 파티션을 생성하고 30일 된 파티션을 삭제합니다.
파티션 관리는 완전히 자동화했습니다. 매일 새벽 2시에 실행되는 배치 작업이 새 파티션 생성과 오래된 파티션 삭제를 담당합니다.

정규화 vs 성능 트레이드오프
"비즈니스 요구사항에 따라 선택적으로 적용했습니다"
기본 마스터 데이터는 3NF를 완전히 준수했습니다. 사용자 정보, 방송 정보, 카테고리 정보는 데이터 일관성이 성능보다 중요하기 때문입니다.

반면 실시간 통계 테이블들은 의도적으로 비정규화했습니다. current_viewers 테이블에는 현재 시청자 수, 최고 시청자 수, 총 시청자 수를 모두 저장하여 조회 시 JOIN 없이 한 번에 모든 정보를 가져올 수 있도록 했습니다.

일별 통계 테이블도 완전히 비정규화하여 복잡한 집계 쿼리 없이 바로 통계 데이터를 조회할 수 있도록 설계했습니다.


## 6. 시스템 레벨 성능 최적화 (2분)

시스템 레벨에서는 두 가지 핵심 최적화를 했습니다.

먼저 파일 디스크립터 한계 해제입니다. WebSocket 연결 1개당 FD 1개가 필요한데, 서버 1대에 5만 연결이면 최소 5만 FD가 필요합니다. 로그, 파일, 내부 파이프까지 고려하면 여유분 포함 10만개를 잡는 게 안전해요. 리눅스 기본 ulimit -n은 1024개라서, 이걸 100000으로 늘렸습니다.

두 번째는 TCP 소켓 버퍼 최적화입니다. 리눅스 기본값은 128KB인데, 연결 1개당 송수신 버퍼 합쳐서 256KB가 커널 메모리에서 고정으로 잡힙니다. 50,000개 연결이면 12.2GB나 되죠.

하지만 실시간 시청자수 집계는 메시지 크기가 매우 작습니다. 보통 수십에서 수백 바이트 수준이에요. 그래서 버퍼를 8KB로 줄였습니다. 그러면 50,000 × 16KB = 0.76GB로 16배나 메모리를 절약할 수 있어요.

MySQL InnoDB 최적화도 했습니다. innodb_buffer_pool_size는 RAM의 70%인 90GB로 설정하고, 배치 업데이트가 많다 보니 innodb_flush_log_at_trx_commit를 2로 설정해서 성능을 우선했습니다. 그리고 SSD 환경에 맞춰 innodb_io_capacity를 2000으로 설정했어요.



## 7. 장애 복구 전략 (1분)


첫번째는, WebSocket 서버 다운이 가장 빈번한 장애입니다. 다른 서버들이 30초마다 헬스체크를 수행하여 장애를 감지하면, 해당 서버가 관리하던 사용자들을 Redis에서 일괄 제거합니다. 사용자들은 자동으로 다른 서버에 재연결되며, 전체 복구 시간은 1-2분입니다.

두번째는, Redis 클러스터 장애에는 다층 Fallback 시스템으로 대응합니다. Redis Sentinel이 30초~2분 내에 자동 페일오버를 수행하고, 그동안은 PostgreSQL에서 5분 전 데이터를 제공합니다. 최악의 경우 "집계 중" 메시지로 서비스 연속성을 보장합니다.

세번째는 Aurora Writer 장애 대응입니다. Aurora 특화 복구 메커니즘으로 15초 내에 Reader 중 하나를 Writer로 자동 승격시킵니다. 6-way 스토리지 복제로 데이터 무손실을 보장하고, 공유 스토리지라 승격 시간이 최소화됩니다. Slave가 다운되면 다른 Slave로 읽기 트래픽을 자동 라우팅합니다.

네 번째는 실시간 조회수의 정합성 검증과 자동 복구입니다. 솔직히 말씀드리면, 저는 Redis를 실제 운영 환경에서 사용해본 경험이 없어서 이 부분은 개념적 설계로 설명드리겠습니다.

3단계 방어선으로 구성했습니다.

첫 번째 안전장치는 Redis SET의 중복 자동 제거입니다. 동일한 사용자가 새로고침이나 네트워크 재연결로 여러 번 접속해도 SET에는 한 번만 저장됩니다.

퇴장 감지는 WebSocket 연결 상태와 Heartbeat 검증을 동시에 사용합니다. 브라우저 종료나 네트워크 끊김은 즉시 감지되고, 예외 상황은 30초마다 Heartbeat로 확인하여 60초 내에 정리됩니다.

"불일치 정도에 따라 3단계로 나누어 대응합니다"

1단계 경량 복구는 5-10% 불일치에 적용합니다. 유령 사용자들을 배치 단위로 점진적 제거하고, 실시간 브로드캐스트로 정정된 시청자 수를 전파합니다. 사용자는 복구 과정을 거의 인지하지 못하며 30초~2분 내에 완료됩니다.

2단계 중간 복구는 10-30% 불일치에 적용합니다. 해당 방송의 시청자수 업데이트를 일시 중단하고 "동기화 중" 메시지를 표시한 뒤, WebSocket 기준으로 Redis 데이터를 완전 재구성합니다. 2-10분 정도 소요됩니다.

3단계 전면 복구는 30% 이상 불일치에 적용합니다. 전체 시스템의 시청자수 기능을 일시 중단하고, 모든 WebSocket 서버에서 실제 연결 상태를 수집하여 Redis를 전면 재작성합니다. 10-30분 소요되지만 다른 기능은 정상 동작합니다.

실제로는 Redis 운영 경험이 있는 시니어 개발자나 Redis 커뮤니티의 베스트 프랙티스를 참고해서 더 구체적인 구현 방법을 결정해야 할 것 같습니다.


8. 확장성 설계

"선형 확장이 가능한 구조로 설계했습니다"
현재 100만 동시접속을 20대 서버로 처리하므로, 500만이 되면 100대 서버로 선형 확장이 가능합니다. 각 서버는 완전히 독립적으로 동작하므로 새 서버 추가만으로 용량을 늘릴 수 있습니다.

지역별 분산도 고려했습니다. 글로벌 서비스로 확장할 때는 아시아, 북미, 유럽에 각각 독립적인 클러스터를 구축하여 지연시간을 최소화할 수 있습니다.

데이터베이스는 샤딩 전략으로 확장합니다. stream_id를 기준으로 해시 분산하여 여러 DB에 분산 저장하고,  애플리케이션 레벨에서 적절한 샤드로 라우팅합니다.

자동 스케일링도 구현했습니다. 서버별 연결 수가 45,000개를 넘으면 자동으로 새 서버를 추가하고, 트래픽이 줄어들면 점진적으로 서버를 제거합니다.
