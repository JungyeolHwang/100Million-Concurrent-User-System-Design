100만 동시접속 실시간 시청자수 시스템 - DBA 면접 발표 상세 가이드

## 면접 질문

100만 동시 접속 환경에서 실시간 조회수 집계 방법 설계
특정 콘텐츠(예: 게시물 등 )의 조회수를 실시간으로 집계하여 DB에 저장해야합니다.
100만 동시 접속자가 존재하는 환경에서 효율적으로 조회수를 저장하고 관리할 방법을 설계하세요.
성능, 확장성, 데이터 정확성을 모두 고려해야 하며 효율적인 데이터 설계 와 구성 방안을 생각해주세요.


## 답변 발표 

🏗️ 1. 아키텍처 소개 (3분)
전체 시스템 구성 설명

"100만 동시접속을 처리하기 위해 3층 구조로 설계했습니다"
첫 번째 층은 WebSocket 서버 풀입니다. 20대의 서버가 각각 5만 연결을 담당하여 총 100만 연결을 처리합니다. 로드밸런서가 사용자를 적절히 분산시키되, 같은 사용자는 같은 서버로 연결되도록 Session Affinity를 적용했습니다.
두 번째 층은 Redis 클러스터입니다. 12개 노드로 구성하여 실시간 데이터를 처리합니다. 여기서 실제 시청자 수 계산과 중복 제거가 이뤄집니다. Redis를 선택한 이유는 초당 10만 TPS 이상의 처리 능력과 1ms 이내의 응답 속도 때문입니다.
세 번째 층은 MySQL 데이터베이스입니다. Master-Slave 구조로 고가용성을 확보하고, 읽기 전용 복제본 3대를 통해 조회 부하를 분산시켰습니다. 실시간 데이터는 5분마다 배치로 업데이트하여 부하를 분산시킵니다.
DBA 관점에서의 핵심 설계 원칙

"MySQL의 역할을 명확히 정의했습니다"
MySQL은 실시간 처리가 아닌 안정적인 데이터 저장과 복잡한 분석 쿼리 처리에 집중합니다. 실시간 업데이트는 Redis가 담당하고, MySQL은 배치 처리로 부하를 분산시켜 안정성을 확보했습니다.
읽기와 쓰기를 분리하여 Master는 배치 업데이트만, Slave들은 각각 랭킹 조회, 히스토리 조회, 분석 쿼리를 전담하도록 역할을 나누었습니다.

🗃️ 2. 데이터베이스 설계 (4분)
테이블 구조 설계 철학

"실시간성과 정확성의 균형을 맞췄습니다"
총 8개의 핵심 테이블로 구성했습니다. users, broadcasters, streams, categories는 기본 마스터 데이터로 완전히 정규화했습니다. 반면 current_viewers와 daily_statistics는 성능을 위해 의도적으로 비정규화를 적용했습니다.
가장 중요한 테이블인 current_viewers는 stream_id당 하나의 레코드만 가지며, 실시간 시청자 수와 최고 시청자 수를 저장합니다. 이 테이블은 초당 수천 번 업데이트되므로 단순한 구조로 설계했습니다.
viewer_history 테이블은 5분마다 시청자 수 변화를 기록하는 시계열 데이터입니다. 월별 파티셔닝을 적용하여 12개월치 데이터를 효율적으로 관리합니다.
인덱스 전략

"조회 패턴을 분석하여 최적화했습니다"
가장 중요한 인덱스는 실시간 랭킹 조회용 복합 인덱스입니다. status, viewer_count DESC, started_at DESC 순으로 구성하여 "현재 라이브 중인 방송을 시청자 수 순으로" 조회하는 메인 화면 쿼리를 최적화했습니다.
커버링 인덱스도 적극 활용했습니다. 자주 조회되는 컬럼들을 인덱스에 포함시켜 테이블 접근 없이 인덱스만으로 결과를 반환할 수 있도록 했습니다.
방송인별 히스토리 조회를 위해서는 broadcaster_id와 recorded_at을 조합한 복합 인덱스를 생성했습니다. 카디널리티가 높은 컬럼을 앞에 배치하여 인덱스 효율성을 극대화했습니다.
파티셔닝 전략

"대용량 데이터를 효율적으로 관리하기 위해 시계열 파티셔닝을 적용했습니다"
viewer_history는 월별 Range 파티셔닝을 적용했습니다. 매월 수억 건의 레코드가 쌓이므로 월단위로 분할하여 쿼리 성능을 유지합니다. 12개월이 지난 파티션은 자동으로 삭제하는 프로시저를 구축했습니다.
user_sessions는 일별 파티셔닝을 적용했습니다. GDPR 규정에 따라 30일만 보관하므로 매일 새 파티션을 생성하고 30일 된 파티션을 삭제합니다.
파티션 관리는 완전히 자동화했습니다. 매일 새벽 2시에 실행되는 배치 작업이 새 파티션 생성과 오래된 파티션 삭제를 담당합니다.
정규화 vs 성능 트레이드오프
"비즈니스 요구사항에 따라 선택적으로 적용했습니다"
기본 마스터 데이터는 3NF를 완전히 준수했습니다. 사용자 정보, 방송 정보, 카테고리 정보는 데이터 일관성이 성능보다 중요하기 때문입니다.
반면 실시간 통계 테이블들은 의도적으로 비정규화했습니다. current_viewers 테이블에는 현재 시청자 수, 최고 시청자 수, 총 시청자 수를 모두 저장하여 조회 시 JOIN 없이 한 번에 모든 정보를 가져올 수 있도록 했습니다.
일별 통계 테이블도 완전히 비정규화하여 복잡한 집계 쿼리 없이 바로 통계 데이터를 조회할 수 있도록 설계했습니다.


🚀 3. Redis 활용 전략 (2분)
MySQL만으로는 한계가 있는 이유

"동시 쓰기 성능의 물리적 한계 때문입니다"
MySQL은 ACID 특성을 보장하기 위해 트랜잭션 로그를 디스크에 써야 합니다. 아무리 SSD를 사용해도 물리적인 I/O 한계가 있어서 초당 5,000 TPS 정도가 현실적 한계입니다.
또한 100만 동시접속에서 발생하는 수많은 입장/퇴장 이벤트를 실시간으로 처리하려면 락 경합이 심각해집니다. 특히 인기 방송의 경우 동시에 수천 명이 입퇴장하는데, MySQL의 행 레벨 락으로는 처리가 어렵습니다.
응답 시간도 문제입니다. 사용자가 방송에 접속하면 즉시 정확한 시청자 수를 보여줘야 하는데, MySQL은 평균 100ms 이상의 지연이 발생합니다.
Redis의 핵심 장점

"메모리 기반 처리와 원자적 연산이 핵심입니다"
Redis는 모든 데이터를 메모리에 저장하므로 디스크 I/O가 없어 초당 10만 TPS 이상 처리가 가능합니다. 평균 응답 시간도 1ms 이내로 실시간 서비스에 최적화되어 있습니다.
가장 중요한 것은 SET 자료구조입니다. 시청자 목록을 SET으로 관리하면 동일한 사용자가 여러 번 추가되어도 자동으로 중복이 제거됩니다. 이를 통해 복잡한 중복 방지 로직 없이도 정확한 시청자 수를 유지할 수 있습니다.
Redis의 원자적 연산도 중요합니다. SADD, SREM, SCARD 명령어는 모두 원자적으로 실행되므로 동시성 문제 없이 안전하게 시청자 수를 관리할 수 있습니다.
MySQL과 Redis의 역할 분담

"각각의 강점을 살린 협업 구조입니다"
Redis는 실시간 데이터 처리를 담당합니다. 사용자 입장/퇴장 즉시 반영하고, 실시간 조회수 표시, 중복 방지 등 1ms 이내 응답이 필요한 모든 작업을 처리합니다.
MySQL은 안정적인 데이터 저장과 복잡한 분석을 담당합니다. Redis의 실시간 데이터를 5분마다 배치로 받아서 영구 저장하고, 시간별/일별 통계 분석, 트렌드 분석, 수익 정산 등을 처리합니다.
데이터 흐름은 단방향입니다. 실시간 변경사항은 Redis에서 시작되어 MySQL로 전파됩니다. 이렇게 하면 데이터 일관성을 유지하면서도 각 시스템의 강점을 최대한 활용할 수 있습니다.

⚡ 4. 100만 동시접속 대응 (3분)

성능 최적화 전략

"병목 지점을 사전에 식별하고 최적화했습니다"
가장 큰 병목은 OS 레벨의 파일 디스크립터 한계입니다. Linux 기본값이 1,024개인데 서버당 5만 연결을 처리하려면 10만 개 이상 필요합니다. 시스템 설정을 변경하여 이 한계를 해제했습니다.
TCP 소켓 버퍼 크기 최적화도 중요합니다. 기본 128KB를 8KB로 줄여서 메모리 사용량을 16배 절약했습니다. 실시간 시청자수는 작은 메시지만 주고받으므로 작은 버퍼로도 충분합니다.
MySQL에서는 InnoDB 엔진 최적화가 핵심입니다. 버퍼 풀을 전체 메모리의 70%로 설정하고, 로그 파일 크기를 적절히 조정하여 배치 업데이트 성능을 극대화했습니다.
Redis 클러스터는 샤딩 전략으로 부하를 분산했습니다. stream_id를 해시하여 12개 노드에 균등 분산시키고, 각 노드마다 슬레이브를 구성하여 읽기 부하도 분산시켰습니다.

정확성 보장 메커니즘

"이중 안전장치로 100% 정확성을 달성했습니다"
첫 번째 안전장치는 Redis SET의 중복 자동 제거입니다. 동일한 사용자가 새로고침이나 네트워크 재연결로 여러 번 접속해도 SET에는 한 번만 저장됩니다.
두 번째 안전장치는 24시간 중복 방지 키입니다. 사용자가 오늘 이미 해당 방송을 조회했다면 별도 키로 추적하여 중복 카운팅을 완전히 차단합니다.
퇴장 감지는 WebSocket 연결 상태와 Heartbeat 검증을 동시에 사용합니다. 브라우저 종료나 네트워크 끊김은 즉시 감지되고, 예외 상황은 30초마다 Heartbeat로 확인하여 60초 내에 정리됩니다.
데이터 정합성 검증도 자동화했습니다. 1분마다 WebSocket 서버의 실제 연결 수와 Redis 데이터를 비교하고, 10% 이상 차이가 나면 자동 복구 프로세스를 시작합니다.

“실시간 동접은 유저 Set으로 중복 없이 집계하고, 퇴장은 WebSocket close + Heartbeat TTL로 최대 60초 내 정리합니다. 
 또 1분마다 실연결 vs Redis를 비교해 10% 이상 어긋나면 세션 진실원장 기준으로 자동 복구합니다.”

확장성 설계

"선형 확장이 가능한 구조로 설계했습니다"
현재 100만 동시접속을 20대 서버로 처리하므로, 500만이 되면 100대 서버로 선형 확장이 가능합니다. 각 서버는 완전히 독립적으로 동작하므로 새 서버 추가만으로 용량을 늘릴 수 있습니다.
지역별 분산도 고려했습니다. 글로벌 서비스로 확장할 때는 아시아, 북미, 유럽에 각각 독립적인 클러스터를 구축하여 지연시간을 최소화할 수 있습니다.
데이터베이스는 샤딩 전략으로 확장합니다. stream_id를 기준으로 해시 분산하여 여러 DB에 분산 저장하고, 애플리케이션 레벨에서 적절한 샤드로 라우팅합니다.
자동 스케일링도 구현했습니다. 서버별 연결 수가 45,000개를 넘으면 자동으로 새 서버를 추가하고, 트래픽이 줄어들면 점진적으로 서버를 제거합니다.

🛡️ 5. 장애 복구 방안 (2분)

주요 장애 시나리오별 대응
"실제 운영에서 발생 가능한 모든 시나리오를 고려했습니다"
WebSocket 서버 다운이 가장 빈번한 장애입니다. 다른 서버들이 30초마다 헬스체크를 수행하여 장애를 감지하면, 해당 서버가 관리하던 사용자들을 Redis에서 일괄 제거합니다. 사용자들은 자동으로 다른 서버에 재연결되며, 전체 복구 시간은 1-2분입니다.
Redis 클러스터 장애에는 다층 Fallback 시스템으로 대응합니다. Redis Sentinel이 30초~2분 내에 자동 페일오버를 수행하고, 그동안은 PostgreSQL에서 5분 전 데이터를 제공합니다. 최악의 경우 로컬 메모리 캐시나 "집계 중" 메시지로 서비스 연속성을 보장합니다.
MySQL 장애는 Master-Slave 구조로 대응합니다. Master가 다운되면 1-2분 내에 Slave를 Master로 승격시키고, Slave가 다운되면 다른 Slave로 읽기 트래픽을 자동 라우팅합니다.

데이터 복구 전략
"불일치 정도에 따라 3단계로 나누어 대응합니다"
1단계 경량 복구는 5-15% 불일치에 적용합니다. 유령 사용자들을 배치 단위로 점진적 제거하고, 실시간 브로드캐스트로 정정된 시청자 수를 전파합니다. 사용자는 복구 과정을 거의 인지하지 못하며 30초~2분 내에 완료됩니다.
2단계 중간 복구는 15-30% 불일치에 적용합니다. 해당 방송의 시청자수 업데이트를 일시 중단하고 "동기화 중" 메시지를 표시한 뒤, WebSocket 기준으로 Redis 데이터를 완전 재구성합니다. 2-10분 정도 소요됩니다.
3단계 전면 복구는 30% 이상 불일치에 적용합니다. 전체 시스템의 시청자수 기능을 일시 중단하고, 모든 WebSocket 서버에서 실제 연결 상태를 수집하여 Redis를 전면 재작성합니다. 10-30분 소요되지만 다른 기능은 정상 동작합니다.